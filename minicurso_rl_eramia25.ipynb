{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasAlegre/minicurso-rl-eramia25/blob/main/minicurso_rl_eramia25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUofEn1oHUvj"
      },
      "source": [
        "# Aprendizado por ReforÃ§o: Como Ensinar RobÃ´s a Maximizar Recompensas na PrÃ¡tica\n",
        "\n",
        "Minicurso realizado no dia 12 de novembro durante o ERAMIA 2025 no Instituto de InformÃ¡tica da UFRGS.\n",
        "\n",
        "Autor: Lucas N. Alegre\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelando Problemas com Gymnasium\n",
        "\n",
        "Gymnasium Ã© a versÃ£o mantida do OpenAI Gym pela Farama Foundation. Ã‰ uma biblioteca Python para desenvolvimento e comparaÃ§Ã£o de algoritmos de Reinforcement Learning.\n",
        "\n",
        "### InstalaÃ§Ã£o\n",
        "\n",
        "```bash\n",
        "pip install gymnasium\n",
        "pip install gymnasium[all]  # Para ambientes adicionais\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-IXG_svtHReK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Definindo um Agente AleatÃ³rio\n",
        "class RandomAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "    def eval(self, obs):\n",
        "        return self.env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation: 0\n",
            "Info: {'prob': 1}\n",
            "EpisÃ³dio terminou! Recompensa: 0\n"
          ]
        }
      ],
      "source": [
        "## 1. Estrutura BÃ¡sica de um Ambiente\n",
        "\n",
        "# Criar ambiente\n",
        "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
        "\n",
        "# Resetar ambiente\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "agent = RandomAgent(env)\n",
        "\n",
        "print(f\"Observation: {observation}\")\n",
        "print(f\"Info: {info}\")\n",
        "\n",
        "# Loop de interaÃ§Ã£o\n",
        "for _ in range(100):\n",
        "    # Escolher aÃ§Ã£o (aleatÃ³ria neste exemplo)\n",
        "    action = agent.eval(observation)\n",
        "    \n",
        "    # Executar aÃ§Ã£o\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    \n",
        "    # Verificar se episÃ³dio terminou\n",
        "    if terminated or truncated:\n",
        "        print(f\"EpisÃ³dio terminou! Recompensa: {reward}\")\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Discrete(16)\n",
            "NÃºmero de estados: 16\n",
            "\n",
            "Action space: Discrete(4)\n",
            "NÃºmero de aÃ§Ãµes: 4\n",
            "AÃ§Ãµes: 0=Esquerda, 1=Baixo, 2=Direita, 3=Cima\n"
          ]
        }
      ],
      "source": [
        "## 2. Componentes Principais\n",
        "\n",
        "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
        "\n",
        "# Observation Space\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"NÃºmero de estados: {env.observation_space.n}\")\n",
        "\n",
        "# Action Space\n",
        "print(f\"\\nAction space: {env.action_space}\")\n",
        "print(f\"NÃºmero de aÃ§Ãµes: {env.action_space.n}\")\n",
        "print(f\"AÃ§Ãµes: 0=Esquerda, 1=Baixo, 2=Direita, 3=Cima\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game over! Your agent lasted 13 steps.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_93885/2810858833.py:22: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
            "  axes[1].set_ylim(0, max(returns) * 1.2)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'HTML' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manimate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36manimate_agent\u001b[39m\u001b[34m(agent, env, num_frames)\u001b[39m\n\u001b[32m     35\u001b[39m anim = animation.FuncAnimation(fig, animate, init_func=init, frames=num_frames,\n\u001b[32m     36\u001b[39m                                interval=\u001b[32m50\u001b[39m)\n\u001b[32m     37\u001b[39m plt.close()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHTML\u001b[49m(anim.to_jshtml())\n",
            "\u001b[31mNameError\u001b[39m: name 'HTML' is not defined"
          ]
        }
      ],
      "source": [
        "animate_agent(agent, env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recursos Adicionais\n",
        "\n",
        "- ðŸ“š DocumentaÃ§Ã£o oficial: https://gymnasium.farama.org/\n",
        "- ðŸŽ® Lista de ambientes: https://gymnasium.farama.org/environments/\n",
        "- ðŸ’» GitHub: https://github.com/Farama-Foundation/Gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dXG0L_7IE_5"
      },
      "source": [
        "## Avaliando um Agente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDfaPBqbK3Bi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g5JPZzTPI2oQ"
      },
      "outputs": [],
      "source": [
        "# @title Animando o agente\n",
        "from matplotlib import animation\n",
        "\n",
        "def animate_agent(agent, env, num_frames=100):\n",
        "  s = env.reset()\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "  im = axes[0].imshow(env.render())\n",
        "  frames = [env.render()]\n",
        "  returns = [0]\n",
        "  env_active = True\n",
        "  for step in range(num_frames):\n",
        "    a = agent.eval(s)\n",
        "    s, r, terminated, truncated, info = env.step(a)\n",
        "    done = terminated or truncated\n",
        "    frames.append(env.render())\n",
        "    returns.append(r + returns[-1])\n",
        "    if env_active and done:\n",
        "      env_active = False\n",
        "      print(f'Game over! Your agent lasted {step} steps.')\n",
        "  axes[1].set_title('Cumulative returns', fontsize=20)\n",
        "  axes[1].set_xlim(0, num_frames)\n",
        "  axes[1].set_ylim(0, max(returns) * 1.2)\n",
        "  line, = axes[1].plot([], [], lw=2)\n",
        "\n",
        "  def init():\n",
        "    line.set_data([], [])\n",
        "    im.set_data(frames[0])\n",
        "    return [im]\n",
        "\n",
        "  def animate(i):\n",
        "    line.set_data(np.arange(i), returns[:i])\n",
        "    im.set_data(frames[i])\n",
        "    return [im]\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, init_func=init, frames=num_frames,\n",
        "                                 interval=50)\n",
        "  plt.close()\n",
        "  return HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEjrXq41IMVt"
      },
      "source": [
        "# Navegando no GridWorld com Q-Learning Tabular\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhB5TbrdJ18W"
      },
      "source": [
        "## Deep Q-Networks\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMltAvQsZNM9EEdqx0IcOuH",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "minicurso-rl-eramia25",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

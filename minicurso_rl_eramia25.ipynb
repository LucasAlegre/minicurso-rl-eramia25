{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMltAvQsZNM9EEdqx0IcOuH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasAlegre/minicurso-rl-eramia25/blob/main/minicurso_rl_eramia25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aprendizado por Reforço: Como Ensinar Robôs a Maximizar Recompensas na Prática\n",
        "\n",
        "Minicurso realizado no dia 12 de novembro durante o ERAMIA 2025 no Instituto de Informática da UFRGS.\n",
        "\n",
        "Autor: Lucas N. Alegre\n"
      ],
      "metadata": {
        "id": "nUofEn1oHUvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelando Problemas com Gymnasium\n"
      ],
      "metadata": {
        "id": "6VVcK9LJH1k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "-IXG_svtHReK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliando um Agente\n"
      ],
      "metadata": {
        "id": "9dXG0L_7IE_5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDfaPBqbK3Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5JPZzTPI2oQ"
      },
      "source": [
        "# @title Animando o agente\n",
        "from matplotlib import animation\n",
        "\n",
        "def animate_agent(agent, env, num_frames=100):\n",
        "  s = env.reset()\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "  im = axes[0].imshow(env.render(mode='rgb_array'))\n",
        "  frames = [env.render(mode='rgb_array')]\n",
        "  returns = [0]\n",
        "  env_active = True\n",
        "  for step in range(num_frames):\n",
        "    a = agent(s)\n",
        "    s, r, done, _ = env.step(a)\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    returns.append(r + returns[-1])\n",
        "    if env_active and done:\n",
        "      env_active = False\n",
        "      print(f'Game over! Your agent lasted {step} steps.')\n",
        "  axes[1].set_title('Cumulative returns', fontsize=20)\n",
        "  axes[1].set_xlim(0, num_frames)\n",
        "  axes[1].set_ylim(0, max(returns) * 1.2)\n",
        "  line, = axes[1].plot([], [], lw=2)\n",
        "\n",
        "  def init():\n",
        "    line.set_data([], [])\n",
        "    im.set_data(frames[0])\n",
        "    return [im]\n",
        "\n",
        "  def animate(i):\n",
        "    line.set_data(onp.arange(i), returns[:i])\n",
        "    im.set_data(frames[i])\n",
        "    return [im]\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, init_func=init, frames=num_frames,\n",
        "                                 interval=50)\n",
        "  plt.close()\n",
        "  return HTML(anim.to_jshtml())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Navegando no GridWorld com Q-Learning Tabular\n"
      ],
      "metadata": {
        "id": "PEjrXq41IMVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Networks\n"
      ],
      "metadata": {
        "id": "FhB5TbrdJ18W"
      }
    }
  ]
}